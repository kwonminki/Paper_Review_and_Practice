# Paper_Review_and_Practice
21.01.30~
* * *
### Image
#### Basic
- Deep Residual Learning for Image Recognition (CVPR 2016) [(paper)](https://arxiv.org/abs/1512.03385)
  + ResNet / [code](https://github.com/kwonminki/Paper_Review_and_Practice/blob/master/Code/ResNet/ResNet.ipynb)
  + ResNet_Framework / [**code**](https://github.com/kwonminki/Paper_Review_and_Practice/tree/master/Code/ResNet_Framework)
- Batch normalization: Accelerating deep network training by reducing internal covariate shift (PMLR 2015) [(paper)](https://arxiv.org/abs/1502.03167)
  + 기본적으로 대부분의 코드에 적용됨.

### Segmentation & Convolution
- U-Net: Convolutional Networks for Biomedical Image Segmentation [(paper)](https://arxiv.org/abs/1505.04597)
  + UNet / [code](https://github.com/kwonminki/Paper_Review_and_Practice/tree/master/Code/UNet_Module)
  + UNet_reggression / [code](https://github.com/kwonminki/Paper_Review_and_Practice/tree/master/Code/UNet_reggression)

#### Instace Segmentation
- YOLACT: Real-time Instance Segmentation [(paper)](https://arxiv.org/abs/1904.02689)
  + YOLACT / [official code](https://github.com/dbolya/yolact) / [발표자료](https://github.com/yj-uh/vi-lab/issues/9)
    * [R-CNN](https://arxiv.org/abs/1311.2524) / [Fast R-CNN](https://arxiv.org/abs/1504.08083) / [Faster R-CNN](https://arxiv.org/abs/1506.01497) / [Mask R-CNN](https://arxiv.org/abs/1703.06870)
  + YOLACT++: Better Real-time Instance Segmentation [(paper)](https://arxiv.org/abs/1912.06218)
  + YolactEdge: Real-time Instance Segmentation on the Edge (Jetson AGX Xavier: 30 FPS, RTX 2080 Ti: 170 FPS) [(paper)](https://arxiv.org/abs/1912.06218)
  
#### GAN
- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (ICLR 2016) [(paper)](https://arxiv.org/abs/1511.06434)
  + DCGAN / [code](https://github.com/kwonminki/Paper_Review_and_Practice/blob/master/Code/DCGAN/DCGAN_Test.ipynb)
  + DCGAN_Framework / [**code**](https://github.com/kwonminki/Paper_Review_and_Practice/tree/master/Code/DCGAN_Framework)
- Image-to-Image Translation with Conditional Adversarial Nets (CVPR 2017) [(paper)](https://phillipi.github.io/pix2pix/)
  + Pix2Pix / [**code**](https://github.com/kwonminki/Paper_Review_and_Practice/tree/master/Code/pix2pix)
    * Pix2PixHD : pix2pixHD: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (CVPR 2018) [(paper)](https://tcwang0509.github.io/pix2pixHD/)
- SPADE: Semantic Image Synthesis With Spatially-Adaptive Normalization (CVPR 2019) [(paper)](https://nvlabs.github.io/SPADE/)
- (CVPR 2018) StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation [(paper)](https://arxiv.org/pdf/1711.09020.pdf)
- **(CVPR 2020) StarGAN v2: Diverse Image Synthesis for Multiple Domains** [(paper)](https://arxiv.org/pdf/1912.01865.pdf)

#### Style Transfer
- Image Style Transfer Using Convolutional Neural Networks(**Gatys**) [(paper)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html)
- Perceptual Losses for Real-Time Style Transfer and Super-Resolution(**Johnson**) [(paper)](https://arxiv.org/abs/1603.08155)
- Instance Normalization: The Missing Ingredient for Fast Stylization [(paper)](https://arxiv.org/abs/1607.08022)
- A Learned Representation For Artistic Style [(paper)](https://arxiv.org/abs/1610.07629)
- Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization(**AdaIN**) [(paper)](https://arxiv.org/abs/1703.06868)

#### Semi-Supervised Learning
- MixMatch: A Holistic Approach to Semi-Supervised Learning [(paper)](https://arxiv.org/abs/1905.02249)
- ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring [(paper)](https://arxiv.org/abs/1911.09785)
- FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence [(paper)](https://arxiv.org/abs/2001.07685)

* * *
### Video
#### GAN
- Adversarial Video Generation on Complex Datasets (ICLR 2020) [(paper)](https://arxiv.org/abs/1907.06571)
  + DVD-GAN / [발표자료](https://github.com/yj-uh/vi-lab/issues/11)
    * [VGAN](https://arxiv.org/abs/1609.02612) / [TGAN](https://arxiv.org/abs/1611.06624) / [MoCoGAN](https://arxiv.org/abs/1707.04993) / [TGANv2](https://arxiv.org/abs/1811.09245)
* * *


### For later
 - Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. 2019. Dig-ging into Self-Supervised Monocular Depth Prediction. InInternational Conferenceon Computer Vision (ICCV)
